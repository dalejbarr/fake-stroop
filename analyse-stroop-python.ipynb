{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Analysis of Stroop Data (Python)\n",
        "author: Dale Barr\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    self-contained: true\n",
        "editor: visual\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{=html}\n",
        "<!--\n",
        "## to run interactively in RStudio, set python install location\n",
        "reticulate::use_python(\"C:/Users/Dale/AppData/Local/Programs/Python/Python310/python.exe\")\n",
        "-->\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: py-interactive-setup\n",
        "#| eval: false\n",
        "#| echo: false\n",
        "\n",
        "## to run interactively, set the working directory in python\n",
        "import os\n",
        "os.chdir(\"c:/Users/Dale/Desktop/fake-stroop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: py-setup\n",
        "from IPython.display import display, Markdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats as st \n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "\n",
        "raw_data_subdir = \"sample\"\n",
        "stroop_colours = set(['blue', 'brown', 'green', 'purple', 'red'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## load in demographics\n",
        "demo_raw = pd.read_csv(os.path.join(raw_data_subdir, \"demographics.csv\"))\n",
        "\n",
        "## use regular expression to identify subject data files\n",
        "rx = re.compile('^S[0-9]{2}\\\\.csv$')\n",
        "\n",
        "files_todo = list(filter(rx.match, os.listdir(raw_data_subdir)))\n",
        "\n",
        "## read in subject data files using a loop\n",
        "li = []\n",
        "for f in files_todo:\n",
        "  df = pd.read_csv(os.path.join(raw_data_subdir, f))\n",
        "  sub_id = re.sub(\"^S([0-9]{2})\\\\.csv$\", \"\\\\1\", f)\n",
        "  li.append(df.assign(id = int(sub_id, base = 10)))\n",
        "\n",
        "trials_raw = pd.concat(li, axis = 0, ignore_index = True)\n",
        "\n",
        "## move 'id' to first column\n",
        "col1 = trials_raw.pop('id')\n",
        "trials_raw.insert(0, 'id', col1)\n",
        "\n",
        "## read in transcript\n",
        "transcript_raw = pd.read_csv(os.path.join(raw_data_subdir, \"transcript.csv\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data cleaning\n",
        "\n",
        "Now let's check any values that were entered by a human for typos and consistency.\n",
        "\n",
        "#### `eng_lang` variable in `demo_raw`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "demo_raw['eng_lang'] = demo_raw['eng_lang'].str.lower()\n",
        "\n",
        "## get rid of bogus ages\n",
        "demo_raw.loc[(demo_raw['age'] < 16) | (demo_raw['age'] > 100), 'age'] = np.NaN\n",
        "\n",
        "## replace 'nativ' with 'native'\n",
        "demo_raw.loc[demo_raw['eng_lang'] == 'nativ', 'eng_lang'] = 'native'\n",
        "\n",
        "el_vals = (\n",
        "  demo_raw\n",
        "  .dropna(subset = ['eng_lang'])\n",
        "  .drop_duplicates(['eng_lang'])\n",
        ")\n",
        "\n",
        "el_vals['eng_lang']\n",
        "\n",
        "if set(el_vals['eng_lang']) != set(['native', 'nonnative']):\n",
        "  sys.exit(\"anomalous values found in 'eng_lang'\")\n",
        "  \n",
        "demo = demo_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `response` variable in `transcript_raw`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_resp = set(transcript_raw['response'])\n",
        "\n",
        "anom_vals = sorted(all_resp.difference(stroop_colours))\n",
        "repaired = ['blue', 'brown', 'brown', 'brown', 'blue', 'green', 'green',\n",
        "            'purple', 'purple', 'purple', 'purple', 'red']\n",
        "\n",
        "transcript_raw['response'] = (transcript_raw['response']\n",
        "                              .replace(anom_vals, repaired))\n",
        "\n",
        "if set(transcript_raw['response']) != stroop_colours:\n",
        "  sys.exit(\"anomalous values detected in transcript `response` column\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trial data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tr = (trials_raw\n",
        "  .query('event == \"DISPLAY_ON\"')\n",
        "  .filter(['id', 'trial', 'data'])\n",
        ")\n",
        "\n",
        "tr[['stimword', 'inkcolour']] = tr['data'].str.split('-', 1, expand=True)\n",
        "\n",
        "tr['inkcolour'] = tr['inkcolour'].apply(lambda x: re.sub(\"\\\\.png$\", \"\", x))\n",
        "\n",
        "tr['condition'] = 'incongruent'\n",
        "tr.loc[tr['stimword'].str.lower() == tr['inkcolour'], \n",
        "       'condition'] = 'congruent'\n",
        "\n",
        "tr2 = tr.drop('data', axis = 1)\n",
        "\n",
        "tr_acc = pd.merge(tr2, transcript_raw, \"left\", on = [\"id\", \"trial\"])\n",
        "\n",
        "trials_acc = (tr_acc\n",
        "              .assign(is_accurate = tr_acc['response'] == tr_acc['inkcolour'])\n",
        "              )\n",
        "\n",
        "trials_rt = (trials_raw\n",
        "  .drop('data', axis = 1)\n",
        "  .pivot(index = ['id', 'trial'], columns = 'event', values = 'timestamp')\n",
        "  .sort_index(level = [1, 0])\n",
        ")\n",
        "\n",
        "trials_rt['rt'] = trials_rt['VOICE_KEY'] - trials_rt['DISPLAY_ON']\n",
        "\n",
        "tr4 = pd.merge(trials_acc, \n",
        "               trials_rt.drop(['DISPLAY_ON', 'VOICE_KEY'], axis = 1),\n",
        "               on = ['id', 'trial'])\n",
        "\n",
        "## perform a 'semi-join' on demo table excluding missing values for eng_lang\n",
        "in_both = tr4['id'].isin(set(demo.query('eng_lang.notnull()')['id']))\n",
        "\n",
        "trials = tr4[in_both]\n",
        "\n",
        "n_trials_wrong = len(trials.query('is_accurate == False').index)\n",
        "n_trials_NA = len(trials.query('rt.isnull()').index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subject means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cb = pd.merge(demo, trials, on = 'id')\n",
        "\n",
        "sub_means = (cb\n",
        "  .query('is_accurate')\n",
        "  .groupby(['id', 'eng_lang', 'condition'])\n",
        "  .agg({'rt' : 'mean'})\n",
        "  .reset_index()\n",
        ")\n",
        "\n",
        "sub_means['rt'] = round(sub_means['rt']).astype('int64')\n",
        "\n",
        "## this is now the 'idealised' data\n",
        "sub_means_wide = (sub_means\n",
        "  .pivot(index = ['id', 'eng_lang'], columns = 'condition', values = 'rt')\n",
        "  .sort_values(['eng_lang', 'id'], ascending = True)\n",
        "  .reset_index()\n",
        "  .rename_axis(None, axis = 1)\n",
        "  .drop('id', axis = 1)\n",
        ")\n",
        "\n",
        "sub_means_wide\n",
        "\n",
        "## make sure we're getting the same subject means that we got from R\n",
        "table_from_r = pd.read_csv(\"subject-means.csv\")\n",
        "\n",
        "if sub_means_wide.equals(table_from_r) == False:\n",
        "  sys.exit(\"'sub_means_wide' did not match result from R\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sub_effects = sub_means_wide.assign(effect = sub_means_wide['incongruent'] - \n",
        "                                      sub_means_wide['congruent'])\n",
        "\n",
        "overall_stroop = sub_effects['effect']\n",
        "\n",
        "result_1s = st.ttest_1samp(overall_stroop, popmean = 0)\n",
        "\n",
        "faster_or_slower = \"faster\" if np.mean(overall_stroop) > 0 else \"slower\"\n",
        "\n",
        "t_crit_1s = st.t.ppf(.975, 39)\n",
        "stderr_1s = np.std(overall_stroop) / math.sqrt(len(overall_stroop) - 1)\n",
        "o_mean = np.mean(overall_stroop)\n",
        "\n",
        "ci_lower_1s = round(o_mean - t_crit_1s * stderr_1s)\n",
        "ci_upper_1s = round(o_mean + t_crit_1s * stderr_1s)\n",
        "\n",
        "native_N = sub_effects.value_counts('eng_lang').get('native')\n",
        "nonnative_N = len(sub_effects) - native_N\n",
        "\n",
        "grp_means = (sub_effects\n",
        "             .groupby('eng_lang')\n",
        "             .agg({'effect' : 'mean'})\n",
        "             .reset_index())\n",
        "\n",
        "grp_sd = (sub_effects\n",
        "             .groupby('eng_lang')\n",
        "             .agg({'effect' : 'std'})\n",
        "             .reset_index())\n",
        "\n",
        "\n",
        "native_eff = grp_means.at[0, 'effect']\n",
        "nonnative_eff = grp_means.at[1, 'effect']\n",
        "\n",
        "native_sd = grp_sd.at[0, 'effect']\n",
        "nonnative_sd = grp_sd.at[1, 'effect']\n",
        "\n",
        "neff = sub_effects.loc[sub_effects['eng_lang'] == 'native', 'effect']\n",
        "nneff = sub_effects.loc[sub_effects['eng_lang'] == 'nonnative', 'effect']\n",
        "\n",
        "ttest_grp = st.ttest_ind(neff, nneff)\n",
        "sig_or_not = \"was\" if ttest_grp.pvalue < .05 else \"was not\"\n",
        "\n",
        "sd_pooled_num = ((native_N - 1) * (native_sd ** 2) + \n",
        "                 (nonnative_N - 1) * (nonnative_sd ** 2))\n",
        "                 \n",
        "sd_pooled = math.sqrt(sd_pooled_num / (native_N + nonnative_N - 2))\n",
        "\n",
        "cohen_d = abs(native_eff - nonnative_eff) / sd_pooled\n",
        "\n",
        "std_err = math.sqrt( (native_sd ** 2) / native_N +\n",
        "                     (nonnative_sd ** 2) / nonnative_N )\n",
        "                     \n",
        "t_crit = st.t.ppf(.975, native_N + nonnative_N - 2)\n",
        "\n",
        "ci_lower = (native_eff - nonnative_eff) - t_crit * std_err\n",
        "ci_upper = (native_eff - nonnative_eff) + t_crit * std_err"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "::: {#results style=\"background-color: rgb(220, 220, 255); padding: 5px;\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "## define some helper functions for displaying text\n",
        "def ul(s, delimit = True):\n",
        "  if delimit:\n",
        "    dtr = \"$\"\n",
        "  else:\n",
        "    dtr = \"\"\n",
        "  is_text = False\n",
        "  if (isinstance(s, str)):\n",
        "    is_text = re.sub(\"\\s\", \"\", s).isalpha()\n",
        "  if is_text:\n",
        "    result = dtr + \"\\\\underline{\\\\text{\" + s + \"}}\" + dtr\n",
        "  else:\n",
        "    result = dtr + \"\\\\underline{\" + str(s) + \"}\" + dtr\n",
        "  return result\n",
        "\n",
        "def perc(num, den, digits = 0):\n",
        "  return round(100 * num / den, digits)\n",
        "\n",
        "## deal with small numbers\n",
        "def si(x):\n",
        "  if (x <= 10):\n",
        "    num_lookup = {\n",
        "      \"1\" : \"one\",\n",
        "      \"2\" : \"two\",\n",
        "      \"3\" : \"three\",\n",
        "      \"4\" : \"four\",\n",
        "      \"5\" : \"five\",\n",
        "      \"6\" : \"six\",\n",
        "      \"7\" : \"seven\",\n",
        "      \"8\" : \"eight\",\n",
        "      \"9\" : \"nine\",\n",
        "      \"10\" : \"ten\"\n",
        "    }\n",
        "    result = num_lookup.get(str(x))\n",
        "  else:\n",
        "    result = str(x)\n",
        "  return result\n",
        "\n",
        "def pvalue_str(pval):\n",
        "  if (pval < .001):\n",
        "    result = \"< .001\"\n",
        "  else:\n",
        "    result = \"=\" + str(round(pval, 3))\n",
        "  return result\n",
        "\n",
        "ngood = len(cb.query('is_accurate & rt.notnull()').index)\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "We ran {npart} participants on a five-colour Stroop task.\n",
        "\"\"\".format(npart = ul(len(demo.index)))))\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "We had to remove data from {badnl} participants whose native language was not properly recorded by the experimenter. From the full set of {nall} trials recorded for the remaining participants, we removed {nwrong} trials ({pwrong}%) where participants produced the wrong answer and {nt_NA} ({pt_NA}%) further trials that could not be analysed because of voice key failure. This left {ngood} trials for analysis. For each participant, we calculated the mean response time in the congruent and incongruent condition.\n",
        "\"\"\".format(badnl = ul(si(len(demo.query('eng_lang.isnull()').index))),\n",
        "           nall = ul(f'{len(cb.index):,}'),\n",
        "           nwrong = ul(si(len(cb.query('is_accurate == False')))),\n",
        "           pwrong = ul(perc(len(cb.query('is_accurate == False')),\n",
        "                            len(cb.index))),\n",
        "           nt_NA = ul(si(len(cb.query('rt.isnull()').index))),\n",
        "           pt_NA = ul(perc(len(cb.query('rt.isnull()').index),\n",
        "                           len(cb.index))),\n",
        "           ngood = ul(f'{ngood:,}')\n",
        "           )))\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "On average, speakers responded {o_mean} milliseconds (SD = {o_sd}) {faster_or_slower} in the congruent than in the incongruent condition, $t({t_df}) = {t_stat}$, $p {pstr}$, $d = {d_eff}$, 95% CI $[{ci_ll}, {ci_ul}]$.\n",
        "\"\"\".format(o_mean = ul(round(np.mean(overall_stroop))),\n",
        "           o_sd = ul(round(np.std(overall_stroop))),\n",
        "           faster_or_slower = ul(faster_or_slower),\n",
        "           t_df = ul(len(overall_stroop) - 1, False),\n",
        "           t_stat = ul(round(result_1s.statistic, 2), False),\n",
        "           pstr = ul(pvalue_str(result_1s.pvalue), False),\n",
        "           d_eff = ul(round(np.mean(overall_stroop) /\n",
        "                            np.std(overall_stroop), 2), False),\n",
        "           ci_ll = ul(ci_lower_1s, False),\n",
        "           ci_ul = ul(ci_upper_1s, False)\n",
        "           )))\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Native English speakers (N = {native_N}) showed an average Stroop effect of {native_eff} ms (SD = {native_sd}), compared to an average of {nonnative_eff} ms (SD = {nonnative_sd}), for non-native English speakers (N = {nonnative_N}). According to a two-tailed independent-samples $t$-test with $\\\\alpha = .05$, the group difference {sig_or_not} statistically significant, $t({t_df})={t_stat}$, $p {t_pval}$, $d = {group_d}$, 95% CI $[{ci_lower}, {ci_upper}]$.\n",
        "\"\"\".format(native_N = ul(native_N),\n",
        "           native_eff = ul(round(native_eff)),\n",
        "           native_sd = ul(round(native_sd)),\n",
        "           nonnative_eff = ul(round(nonnative_eff)),\n",
        "           nonnative_sd = ul(round(nonnative_sd)),\n",
        "           nonnative_N = ul(nonnative_N),\n",
        "           sig_or_not = ul(sig_or_not),\n",
        "           t_df = ul(native_N + nonnative_N - 2, False),\n",
        "           t_stat = ul(round(ttest_grp.statistic, 2), False),\n",
        "           t_pval = ul(pvalue_str(ttest_grp.pvalue), False),\n",
        "           group_d = ul(round(cohen_d, 2), False),\n",
        "           ci_lower = ul(round(ci_lower), False),\n",
        "           ci_upper = ul(round(ci_upper), False)\n",
        ")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}